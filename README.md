# <ins>Этап 5</ins> <br/>Оценка и анализ производительности сервера на базе кластера с репликацией данных
**Система и программные средства** 
| | |
|-|-|
| ОС | Ubuntu Linux 18.04 LTS x64-bit |
| ЦПУ | Intel(R) Celeron(R) N4000 CPU @ 1.10GHz |
| Объём RAM | 8 ГБ |
| Количество ядер ЦПУ | 2 |
| [wrk2](https://github.com/giltene/wrk2) | v. 4.0.0 |
| [async-profiler](https://github.com/jvm-profiling-tools/async-profiler) | 1.8.1 |

Аналитическое задание для отчётного этапа выполнено на основе реализации кластерного highload-сервера с поддержкой репликации данных. Результаты измерений быстродействия при выполнении запросов в **конфигурации с управлением репликами <em>(Replication control)</em>** сопоставлены с выводами нагрузочного тестирования, полученными в **базовом варианте распределённого обслуживания через шардирование <em>(Basic shard control</em>)** согласно задаче предыдущего этапа. Для оценки ключевых параметров производительности - задержки и интенсивности операций сервера, а также времени ожидания отклика на стороне клиента - выбран инструмент симуляции запросов <em>wrk2</em>; профилирование рабочей нагрузки процессора, памяти и программных средств управления параллельным доступом к данным проведено с использованием ПО <em>async-profiler</em>. Релевантные контексту статистика и профили (изображения в стиле flamegraph-визуализации) получены в ходе последовательных сеансов подачи PUT- и GET-запросов в течение фиксированного периода (7 минут) стабильного обмена данными с пулом клиентов (число соединений с клиентами, симулируемых в каждой конфигурации, установлено равным 64, количество параллельных потоков - 2 по числу ядер процессора). Интенсивность формирования / отправки запросов каждого вида (<em>Rate</em>) для базового шардирования задана равной 10000 запросов/с, в кластере с репликами - 5000 запросов/с исходя из предварительных выводов о нагрузке, при которой фактическая производительность симулятора запросов характеризуется устойчивым достижением уровня, эквивалентного или максимально близкого выбранному значению <em>Rate</em>.

**Команды <em>wrk2</em>**<br/>

<p></p>

<ins><em>wrk2</em> / PUT / basic sharding</ins>
```
wrk -t2 -c64 -d7m -s src/profiling/wrk_scripts/put.lua -R10000 --latency http://127.0.0.1:8080
```

<ins><em>wrk2</em> / GET / basic sharding</ins>
```
wrk -t2 -c64 -d7m -s src/profiling/wrk_scripts/get.lua -R10000 --latency http://127.0.0.1:8080
```

<ins><em>wrk2</em> / PUT / replication</ins>
```
wrk -t2 -c64 -d7m -s src/profiling/wrk_scripts/put.lua -R5000 --latency http://127.0.0.1:8080
```

<ins><em>wrk2</em> / GET / replication</ins>
```
wrk -t2 -c64 -d7m -s src/profiling/wrk_scripts/get.lua -R5000 --latency http://127.0.0.1:8080
```

**Команды <em>async-profiler</em>**<br/>

<p></p>

<ins><em>async-profiler</em> / cpu</ins>
```
./profiler.sh -d 60 -e cpu -f /path/to/output/folder/flame_output_cpu.svg <server_process_pid>
```

<ins><em>async-profiler</em> / alloc</ins>
```
./profiler.sh -d 60 -e alloc -f /path/to/output/folder/flame_output_alloc.svg <server_process_pid>
```

<ins><em>async-profiler</em> / lock</ins>
```
./profiler.sh -d 60 -e lock -f /path/to/output/folder/flame_output_lock.svg <server_pid>
```

Результаты измерений и сравнение этапных конфигураций кластера приведены далее.

### 1. Добавление/изменение записей (PUT)

<ins><em>wrk2</em> outputs / basic sharding</ins>  
```
max@max-Inspiron-15-3573:~/hackdht$ wrk -t2 -c64 -d7m -s src/profiling/wrk_scripts/put.lua -R10000 --latency http://127.0.0.1:8080
Running 7m test @ http://127.0.0.1:8080
  2 threads and 64 connections
  Thread calibration: mean lat.: 799.709ms, rate sampling interval: 2326ms
  Thread calibration: mean lat.: 804.702ms, rate sampling interval: 2433ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     5.43ms   14.70ms 272.13ms   96.90%
    Req/Sec     5.00k    51.11     5.46k    92.44%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    2.76ms
 75.000%    3.81ms
 90.000%    6.69ms
 99.000%   87.42ms
 99.900%  183.42ms
 99.990%  224.00ms
 99.999%  258.30ms
100.000%  272.38ms

  Detailed Percentile spectrum:
       Value   Percentile   TotalCount 1/(1-Percentile)

       0.100     0.000000            1         1.00
       1.238     0.100000       410712         1.11
       1.743     0.200000       820161         1.25
       2.125     0.300000      1230426         1.43
       2.447     0.400000      1641444         1.67
       2.759     0.500000      2050806         2.00
       2.925     0.550000      2255987         2.22
       3.105     0.600000      2462100         2.50
       3.301     0.650000      2665682         2.86
       3.529     0.700000      2870918         3.33
       3.805     0.750000      3076515         4.00
       3.973     0.775000      3178580         4.44
       4.175     0.800000      3281629         5.00
       4.423     0.825000      3382938         5.71
       4.763     0.850000      3485497         6.67
       5.323     0.875000      3588041         8.00
       5.835     0.887500      3639385         8.89
       6.691     0.900000      3690440        10.00
       7.823     0.912500      3741665        11.43
       9.071     0.925000      3792933        13.33
      10.599     0.937500      3844275        16.00
      11.535     0.943750      3869725        17.78
      12.647     0.950000      3895464        20.00
      13.967     0.956250      3920998        22.86
      15.839     0.962500      3946681        26.67
      19.855     0.968750      3972237        32.00
      24.623     0.971875      3985047        35.56
      31.103     0.975000      3997867        40.00
      39.199     0.978125      4010713        45.71
      48.735     0.981250      4023490        53.33
      60.703     0.984375      4036333        64.00
      67.583     0.985938      4042736        71.11
      75.135     0.987500      4049136        80.00
      82.751     0.989062      4055527        91.43
      90.495     0.990625      4061943       106.67
      98.431     0.992188      4068343       128.00
     102.271     0.992969      4071585       142.22
     106.047     0.993750      4074747       160.00
     111.295     0.994531      4077948       182.86
     118.527     0.995313      4081168       213.33
     126.911     0.996094      4084370       256.00
     131.583     0.996484      4085954       284.44
     137.343     0.996875      4087571       320.00
     143.871     0.997266      4089174       365.71
     151.807     0.997656      4090772       426.67
     160.255     0.998047      4092381       512.00
     164.223     0.998242      4093173       568.89
     168.319     0.998437      4093966       640.00
     173.183     0.998633      4094781       731.43
     178.431     0.998828      4095572       853.33
     184.063     0.999023      4096371      1024.00
     187.007     0.999121      4096765      1137.78
     189.823     0.999219      4097177      1280.00
     193.279     0.999316      4097567      1462.86
     196.863     0.999414      4097971      1706.67
     200.319     0.999512      4098365      2048.00
     202.111     0.999561      4098579      2275.56
     204.031     0.999609      4098771      2560.00
     206.079     0.999658      4098974      2925.71
     208.383     0.999707      4099170      3413.33
     211.071     0.999756      4099366      4096.00
     212.863     0.999780      4099469      4551.11
     214.655     0.999805      4099567      5120.00
     216.575     0.999829      4099668      5851.43
     218.623     0.999854      4099767      6826.67
     221.567     0.999878      4099869      8192.00
     222.847     0.999890      4099919      9102.22
     224.511     0.999902      4099969     10240.00
     226.175     0.999915      4100018     11702.86
     228.351     0.999927      4100070     13653.33
     231.039     0.999939      4100122     16384.00
     232.447     0.999945      4100143     18204.44
     234.111     0.999951      4100168     20480.00
     236.287     0.999957      4100193     23405.71
     238.591     0.999963      4100217     27306.67
     241.663     0.999969      4100242     32768.00
     243.327     0.999973      4100255     36408.89
     244.607     0.999976      4100267     40960.00
     246.271     0.999979      4100280     46811.43
     248.831     0.999982      4100292     54613.33
     251.519     0.999985      4100305     65536.00
     252.543     0.999986      4100311     72817.78
     254.719     0.999988      4100317     81920.00
     257.023     0.999989      4100324     93622.86
     259.583     0.999991      4100330    109226.67
     260.863     0.999992      4100337    131072.00
     261.375     0.999993      4100339    145635.56
     262.399     0.999994      4100342    163840.00
     263.935     0.999995      4100346    187245.71
     264.959     0.999995      4100350    218453.33
     265.727     0.999996      4100353    262144.00
     265.727     0.999997      4100353    291271.11
     266.751     0.999997      4100355    327680.00
     267.775     0.999997      4100357    374491.43
     268.031     0.999998      4100358    436906.67
     268.287     0.999998      4100360    524288.00
     268.287     0.999998      4100360    582542.22
     268.543     0.999998      4100361    655360.00
     269.567     0.999999      4100362    748982.86
     270.591     0.999999      4100363    873813.33
     271.359     0.999999      4100364   1048576.00
     271.359     0.999999      4100364   1165084.44
     271.359     0.999999      4100364   1310720.00
     271.615     0.999999      4100365   1497965.71
     271.615     0.999999      4100365   1747626.67
     271.871     1.000000      4100366   2097152.00
     271.871     1.000000      4100366   2330168.89
     271.871     1.000000      4100366   2621440.00
     271.871     1.000000      4100366   2995931.43
     271.871     1.000000      4100366   3495253.33
     272.383     1.000000      4100367   4194304.00
     272.383     1.000000      4100367          inf
#[Mean    =        5.431, StdDeviation   =       14.704]
#[Max     =      272.128, Total count    =      4100367]
#[Buckets =           27, SubBuckets     =         2048]
----------------------------------------------------------
  4189947 requests in 7.00m, 331.65MB read
Requests/sec:   9976.03
Transfer/sec:    808.60KB
```
<ins><em>wrk2</em> outputs / replication</ins>  
```
max@max-Inspiron-15-3573:~/hackdht$ wrk -t2 -c64 -d7m -s src/profiling/wrk_scripts/put.lua -R5000 --latency http://127.0.0.1:8080
Running 7m test @ http://127.0.0.1:8080
  2 threads and 64 connections
  Thread calibration: mean lat.: 266.512ms, rate sampling interval: 1072ms
  Thread calibration: mean lat.: 221.330ms, rate sampling interval: 973ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     9.02ms   33.49ms 689.15ms   95.92%
    Req/Sec     2.50k    86.93     2.97k    92.15%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    1.91ms
 75.000%    2.82ms
 90.000%   11.34ms
 99.000%  196.61ms
 99.900%  350.72ms
 99.990%  540.67ms
 99.999%  652.80ms
100.000%  689.66ms

  Detailed Percentile spectrum:
       Value   Percentile   TotalCount 1/(1-Percentile)

       0.253     0.000000            1         1.00
       0.891     0.100000       205204         1.11
       1.179     0.200000       410171         1.25
       1.428     0.300000       615323         1.43
       1.664     0.400000       819768         1.67
       1.906     0.500000      1024809         2.00
       2.036     0.550000      1127383         2.22
       2.177     0.600000      1229966         2.50
       2.337     0.650000      1332386         2.86
       2.535     0.700000      1435305         3.33
       2.815     0.750000      1537029         4.00
       3.031     0.775000      1588444         4.44
       3.365     0.800000      1639360         5.00
       4.079     0.825000      1690624         5.71
       5.499     0.850000      1741896         6.67
       7.719     0.875000      1793070         8.00
       9.255     0.887500      1818692         8.89
      11.343     0.900000      1844284        10.00
      13.943     0.912500      1869934        11.43
      17.151     0.925000      1895497        13.33
      21.519     0.937500      1921160        16.00
      24.607     0.943750      1933930        17.78
      28.943     0.950000      1946757        20.00
      36.799     0.956250      1959561        22.86
      50.175     0.962500      1972340        26.67
      67.519     0.968750      1985165        32.00
      78.975     0.971875      1991567        35.56
      92.671     0.975000      1997967        40.00
     108.671     0.978125      2004380        45.71
     130.815     0.981250      2010763        53.33
     154.367     0.984375      2017177        64.00
     165.503     0.985938      2020368        71.11
     177.407     0.987500      2023616        80.00
     189.183     0.989062      2026793        91.43
     202.239     0.990625      2029981       106.67
     214.143     0.992188      2033193       128.00
     221.055     0.992969      2034782       142.22
     231.807     0.993750      2036385       160.00
     243.711     0.994531      2037984       182.86
     258.047     0.995313      2039581       213.33
     274.687     0.996094      2041184       256.00
     281.599     0.996484      2041983       284.44
     289.279     0.996875      2042792       320.00
     295.679     0.997266      2043582       365.71
     303.359     0.997656      2044387       426.67
     311.295     0.998047      2045194       512.00
     314.623     0.998242      2045613       568.89
     317.951     0.998437      2046002       640.00
     322.303     0.998633      2046397       731.43
     330.751     0.998828      2046788       853.33
     353.791     0.999023      2047184      1024.00
     368.639     0.999121      2047387      1137.78
     385.023     0.999219      2047584      1280.00
     401.663     0.999316      2047784      1462.86
     416.255     0.999414      2047984      1706.67
     431.359     0.999512      2048187      2048.00
     440.319     0.999561      2048284      2275.56
     451.327     0.999609      2048386      2560.00
     462.847     0.999658      2048484      2925.71
     475.647     0.999707      2048585      3413.33
     488.191     0.999756      2048684      4096.00
     493.311     0.999780      2048735      4551.11
     500.223     0.999805      2048785      5120.00
     508.415     0.999829      2048836      5851.43
     515.839     0.999854      2048884      6826.67
     526.335     0.999878      2048934      8192.00
     533.503     0.999890      2048960      9102.22
     543.231     0.999902      2048984     10240.00
     551.935     0.999915      2049009     11702.86
     560.639     0.999927      2049034     13653.33
     568.319     0.999939      2049060     16384.00
     578.047     0.999945      2049072     18204.44
     587.775     0.999951      2049084     20480.00
     593.919     0.999957      2049097     23405.71
     597.503     0.999963      2049109     27306.67
     602.623     0.999969      2049122     32768.00
     605.695     0.999973      2049128     36408.89
     611.839     0.999976      2049134     40960.00
     619.007     0.999979      2049141     46811.43
     623.103     0.999982      2049147     54613.33
     631.807     0.999985      2049153     65536.00
     636.927     0.999986      2049156     72817.78
     645.119     0.999988      2049159     81920.00
     652.799     0.999989      2049165     93622.86
     654.335     0.999991      2049166    109226.67
     664.575     0.999992      2049169    131072.00
     666.111     0.999993      2049170    145635.56
     669.695     0.999994      2049172    163840.00
     674.815     0.999995      2049174    187245.71
     676.863     0.999995      2049175    218453.33
     682.495     0.999996      2049177    262144.00
     682.495     0.999997      2049177    291271.11
     683.519     0.999997      2049178    327680.00
     684.543     0.999997      2049179    374491.43
     685.055     0.999998      2049180    436906.67
     685.567     0.999998      2049181    524288.00
     685.567     0.999998      2049181    582542.22
     685.567     0.999998      2049181    655360.00
     688.639     0.999999      2049183    748982.86
     688.639     0.999999      2049183    873813.33
     688.639     0.999999      2049183   1048576.00
     688.639     0.999999      2049183   1165084.44
     688.639     0.999999      2049183   1310720.00
     688.639     0.999999      2049183   1497965.71
     688.639     0.999999      2049183   1747626.67
     689.663     1.000000      2049184   2097152.00
     689.663     1.000000      2049184          inf
#[Mean    =        9.025, StdDeviation   =       33.491]
#[Max     =      689.152, Total count    =      2049184]
#[Buckets =           27, SubBuckets     =         2048]
----------------------------------------------------------
  2099617 requests in 7.00m, 134.16MB read
Requests/sec:   4999.14
Transfer/sec:    327.09KB
```
Динамика быстродействия узлов кластера, фиксируемая при добавлении контроля репликаций, имеет явный негативный уклон на всём множестве релевантных метрик. При резком (66%) возрастании средней задержки добавления/изменения данных интенсивность обработки соответствующих запросов составила лишь половину от ориентира, полученного в первоначальной реализации шардирования. Признаки значительного ухудшения производительности прослеживаются и в распределении времён отклика: несмотря на определённое (приблизительно 26%) снижение среднего интервала ожидания, достигаемое по итогам обслуживания трёх четвертей запросов, кластер, обеспечивающий хранение реплик, значительно (более чем в 2,5 раза в случае с максимальным по продолжительнсти ответом) уступает результатам наблюдения базовой конфигурации для запросов, составляющих остальные 25% текущего распределения.<br/>            
<ins>Flamegraph-анализ</ins><br/>  

![put_cpu_basic_shards](resources/flamegraphs/put_cpu_basic_shards.svg)
<p align="center">Рис.1. Выделение ресурса CPU при симулировании PUT-запросов (<em>basic sharding</em>)</p>

![put_cpu_replicas](resources/flamegraphs/put_cpu_replicas.svg)
<p align="center">Рис.2. Выделение ресурса CPU при симулировании PUT-запросов (<em>replication</em>)</p>

В структуре профилей процессорного времени для сравниваемых конфигураций заметен ряд различий, главными среди которых следует назвать нивелирование фактора асинхронной обработки запросов и поддержку процедуры создания и сохранения реплик в ходе истытаний кластера с репликацией. Сравнение данных профилей показывает, что ведущими факторами нагрузки на вычислительный ресурс становятся операции чтения и парсинга данных из буфера, в значительной мере дополняемые генерацией и записью реплик в копии LSM-хранилища на узлах кластера. Этой особенностью текущей реализации сервера, предполагающей объективные эффекты в виде накладных расходов на поддержку репликации (с формированием идентичных записей, обменом данными внутри кластера, определением timestamp-значений для каждой реплики), возможно обосновать значительное ухудшение показателей быстродействия в соответствии с вышеупомянутыми результатами нагрузочных тестов. 

![put_alloc_basic_shards](resources/flamegraphs/put_alloc_basic_shards.svg)
<p align="center">Рис.3. Выделение ресурса RAM при симулировании PUT-запросов (<em>basic sharding</em>)</p>

![put_alloc_replicas](resources/flamegraphs/put_alloc_replicas.svg)
<p align="center">Рис.4. Выделение ресурса RAM при симулировании PUT-запросов (<em>replication</em>)</p>

Релевантные изменения в процессах, протекающих на узлах кластера с созданием реплик, проявляются и в разрезе аллокаций. Профиль сервера в новой конфигурации не содержит признаков вызова асинхронного обработчика, составляющего значимую долю выделений памяти в базовом распределении нагрузки. В то же время часть аллокаций выполняется в рамках операций с репликами (включая преобразования между объектными типами), свидетельством чего, как и в представлении процессорного времени, определены фиксируемые вызовы метода <em>upsertWithMultipleNodes</em>.       

![put_lock_basic_shards](resources/flamegraphs/put_lock_basic_shards.svg)
<p align="center">Рис.5. Профиль lock/monitor при симулировании PUT-запросов (<em>basic sharding</em>)</p>

![put_lock_replicas](resources/flamegraphs/put_lock_replicas.svg)
<p align="center">Рис.6. Профиль lock/monitor при симулировании PUT-запросов (<em>replication</em>)</p>

Для управления совместным доступом в контексте обеспечения репликации характерно достижение взаимоисключений в ходе чтения/парсинга данных, операций с созданием/распространением идентичных записей и передачей запроса на следующий узел в рамках распределённой топологии (проксированием). Таким образом, задача синхронизации доступа к блокирующей очереди, относившаяся к числу основных для потокобезопасного выполнения обработчиков и предполагавшая реализацию особого механизма на предыдущем этапе, утратила свою актуальность, что предопределило своеобразное упрощение состава профиля в новой конфигурации.<br/>               

### 2. Чтение записей (GET)

<ins><em>wrk2</em> outputs / basic sharding</ins>  
```
max@max-Inspiron-15-3573:~/hackdht$ wrk -t2 -c64 -d7m -s src/profiling/wrk_scripts/get.lua -R10000 --latency http://127.0.0.1:8080
Running 7m test @ http://127.0.0.1:8080
  2 threads and 64 connections
  Thread calibration: mean lat.: 52.094ms, rate sampling interval: 429ms
  Thread calibration: mean lat.: 22.398ms, rate sampling interval: 188ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     3.98ms   14.30ms 591.36ms   98.75%
    Req/Sec     5.01k   176.33     6.95k    94.22%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    2.58ms
 75.000%    3.52ms
 90.000%    4.79ms
 99.000%   23.39ms
 99.900%  191.49ms
 99.990%  526.34ms
 99.999%  579.58ms
100.000%  591.87ms

  Detailed Percentile spectrum:
       Value   Percentile   TotalCount 1/(1-Percentile)

       0.100     0.000000            6         1.00
       1.071     0.100000       409747         1.11
       1.523     0.200000       819463         1.25
       1.906     0.300000      1228255         1.43
       2.253     0.400000      1638765         1.67
       2.583     0.500000      2046910         2.00
       2.751     0.550000      2251295         2.22
       2.925     0.600000      2455768         2.50
       3.109     0.650000      2661280         2.86
       3.305     0.700000      2865294         3.33
       3.523     0.750000      3070260         4.00
       3.645     0.775000      3171977         4.44
       3.783     0.800000      3274739         5.00
       3.943     0.825000      3377098         5.71
       4.139     0.850000      3480591         6.67
       4.395     0.875000      3582114         8.00
       4.563     0.887500      3632332         8.89
       4.791     0.900000      3683956        10.00
       5.119     0.912500      3734977        11.43
       5.687     0.925000      3785845        13.33
       6.791     0.937500      3836968        16.00
       7.499     0.943750      3862465        17.78
       8.247     0.950000      3888079        20.00
       9.031     0.956250      3913812        22.86
       9.895     0.962500      3939287        26.67
      10.959     0.968750      3964763        32.00
      11.623     0.971875      3977647        35.56
      12.367     0.975000      3990404        40.00
      13.247     0.978125      4003206        45.71
      14.303     0.981250      4015974        53.33
      15.735     0.984375      4028724        64.00
      16.751     0.985938      4035123        71.11
      18.239     0.987500      4041504        80.00
      20.943     0.989062      4047915        91.43
      25.343     0.990625      4054286       106.67
      31.119     0.992188      4060676       128.00
      34.559     0.992969      4063881       142.22
      38.847     0.993750      4067080       160.00
      45.023     0.994531      4070267       182.86
      59.103     0.995313      4073459       213.33
      89.023     0.996094      4076659       256.00
     111.039     0.996484      4078257       284.44
     127.999     0.996875      4079854       320.00
     142.847     0.997266      4081460       365.71
     157.183     0.997656      4083050       426.67
     166.143     0.998047      4084651       512.00
     169.727     0.998242      4085456       568.89
     173.183     0.998437      4086254       640.00
     177.407     0.998633      4087048       731.43
     180.863     0.998828      4087854       853.33
     194.047     0.999023      4088650      1024.00
     201.983     0.999121      4089050      1137.78
     208.127     0.999219      4089453      1280.00
     215.935     0.999316      4089849      1462.86
     242.431     0.999414      4090244      1706.67
     300.031     0.999512      4090644      2048.00
     342.271     0.999561      4090844      2275.56
     381.695     0.999609      4091045      2560.00
     415.999     0.999658      4091244      2925.71
     448.511     0.999707      4091445      3413.33
     468.735     0.999756      4091644      4096.00
     478.975     0.999780      4091747      4551.11
     484.095     0.999805      4091843      5120.00
     489.983     0.999829      4091947      5851.43
     495.103     0.999854      4092048      6826.67
     512.511     0.999878      4092143      8192.00
     522.495     0.999890      4092200      9102.22
     526.847     0.999902      4092243     10240.00
     535.551     0.999915      4092298     11702.86
     542.719     0.999927      4092346     13653.33
     551.423     0.999939      4092403     16384.00
     553.983     0.999945      4092420     18204.44
     557.567     0.999951      4092450     20480.00
     559.615     0.999957      4092468     23405.71
     562.687     0.999963      4092493     27306.67
     566.271     0.999969      4092518     32768.00
     567.807     0.999973      4092533     36408.89
     569.343     0.999976      4092543     40960.00
     571.903     0.999979      4092556     46811.43
     574.463     0.999982      4092572     54613.33
     575.487     0.999985      4092581     65536.00
     575.999     0.999986      4092586     72817.78
     578.047     0.999988      4092596     81920.00
     578.559     0.999989      4092599     93622.86
     580.095     0.999991      4092609    109226.67
     580.607     0.999992      4092611    131072.00
     581.631     0.999993      4092614    145635.56
     582.655     0.999994      4092618    163840.00
     584.191     0.999995      4092624    187245.71
     584.191     0.999995      4092624    218453.33
     584.703     0.999996      4092627    262144.00
     585.215     0.999997      4092628    291271.11
     585.727     0.999997      4092630    327680.00
     586.239     0.999997      4092632    374491.43
     586.751     0.999998      4092633    436906.67
     588.799     0.999998      4092635    524288.00
     588.799     0.999998      4092635    582542.22
     589.311     0.999998      4092636    655360.00
     590.335     0.999999      4092637    748982.86
     590.847     0.999999      4092638    873813.33
     591.359     0.999999      4092640   1048576.00
     591.359     0.999999      4092640   1165084.44
     591.359     0.999999      4092640   1310720.00
     591.359     0.999999      4092640   1497965.71
     591.359     0.999999      4092640   1747626.67
     591.871     1.000000      4092642   2097152.00
     591.871     1.000000      4092642          inf
#[Mean    =        3.977, StdDeviation   =       14.305]
#[Max     =      591.360, Total count    =      4092642]
#[Buckets =           27, SubBuckets     =         2048]
----------------------------------------------------------
  4188850 requests in 7.00m, 337.44MB read
  Non-2xx or 3xx responses: 1693
Requests/sec:   9973.48
Transfer/sec:    822.71KB
```
<ins><em>wrk2</em> outputs / replication</ins>  
```
max@max-Inspiron-15-3573:~/hackdht$ wrk -t2 -c64 -d7m -s src/profiling/wrk_scripts/get.lua -R5000 --latency http://127.0.0.1:8080
Running 7m test @ http://127.0.0.1:8080
  2 threads and 64 connections
  Thread calibration: mean lat.: 164.216ms, rate sampling interval: 856ms
  Thread calibration: mean lat.: 173.495ms, rate sampling interval: 879ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     2.83ms   11.82ms 691.71ms   98.52%
    Req/Sec     2.50k    28.24     2.85k    95.44%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    1.54ms
 75.000%    2.14ms
 90.000%    3.34ms
 99.000%   20.01ms
 99.900%  168.83ms
 99.990%  540.16ms
 99.999%  664.58ms
100.000%  692.22ms

  Detailed Percentile spectrum:
       Value   Percentile   TotalCount 1/(1-Percentile)

       0.212     0.000000            1         1.00
       0.735     0.100000       205256         1.11
       0.964     0.200000       409932         1.25
       1.158     0.300000       614860         1.43
       1.346     0.400000       820320         1.67
       1.540     0.500000      1025553         2.00
       1.642     0.550000      1127501         2.22
       1.749     0.600000      1229894         2.50
       1.864     0.650000      1332101         2.86
       1.991     0.700000      1434916         3.33
       2.137     0.750000      1537206         4.00
       2.221     0.775000      1589193         4.44
       2.315     0.800000      1639740         5.00
       2.429     0.825000      1690817         5.71
       2.583     0.850000      1742240         6.67
       2.819     0.875000      1793128         8.00
       3.017     0.887500      1818877         8.89
       3.339     0.900000      1844332        10.00
       3.919     0.912500      1869936        11.43
       4.743     0.925000      1895573        13.33
       5.687     0.937500      1921190        16.00
       6.247     0.943750      1933993        17.78
       6.883     0.950000      1946779        20.00
       7.627     0.956250      1959572        22.86
       8.583     0.962500      1972384        26.67
       9.847     0.968750      1985163        32.00
      10.551     0.971875      1991602        35.56
      11.327     0.975000      1997996        40.00
      12.151     0.978125      2004407        45.71
      13.055     0.981250      2010798        53.33
      14.255     0.984375      2017192        64.00
      15.119     0.985938      2020390        71.11
      16.303     0.987500      2023579        80.00
      18.207     0.989062      2026792        91.43
      21.551     0.990625      2029979       106.67
      26.751     0.992188      2033180       128.00
      30.575     0.992969      2034780       142.22
      35.807     0.993750      2036384       160.00
      43.743     0.994531      2037987       182.86
      54.207     0.995313      2039585       213.33
      67.647     0.996094      2041190       256.00
      75.263     0.996484      2041984       284.44
      86.399     0.996875      2042787       320.00
     101.567     0.997266      2043587       365.71
     118.655     0.997656      2044387       426.67
     137.215     0.998047      2045189       512.00
     145.535     0.998242      2045593       568.89
     153.983     0.998437      2045993       640.00
     159.103     0.998633      2046394       731.43
     164.351     0.998828      2046792       853.33
     169.343     0.999023      2047192      1024.00
     171.775     0.999121      2047390      1137.78
     175.231     0.999219      2047596      1280.00
     178.431     0.999316      2047798      1462.86
     182.015     0.999414      2047989      1706.67
     186.367     0.999512      2048192      2048.00
     188.927     0.999561      2048293      2275.56
     192.255     0.999609      2048390      2560.00
     197.375     0.999658      2048488      2925.71
     204.031     0.999707      2048588      3413.33
     239.615     0.999756      2048688      4096.00
     311.039     0.999780      2048738      4551.11
     374.015     0.999805      2048788      5120.00
     427.519     0.999829      2048838      5851.43
     468.735     0.999854      2048889      6826.67
     493.823     0.999878      2048938      8192.00
     517.375     0.999890      2048963      9102.22
     545.279     0.999902      2048988     10240.00
     567.807     0.999915      2049013     11702.86
     585.215     0.999927      2049038     13653.33
     601.087     0.999939      2049063     16384.00
     606.207     0.999945      2049076     18204.44
     614.399     0.999951      2049088     20480.00
     619.519     0.999957      2049101     23405.71
     627.199     0.999963      2049113     27306.67
     637.439     0.999969      2049126     32768.00
     639.999     0.999973      2049133     36408.89
     645.631     0.999976      2049138     40960.00
     649.215     0.999979      2049145     46811.43
     652.799     0.999982      2049152     54613.33
     656.383     0.999985      2049157     65536.00
     658.431     0.999986      2049161     72817.78
     660.479     0.999988      2049163     81920.00
     664.063     0.999989      2049167     93622.86
     665.087     0.999991      2049170    109226.67
     667.647     0.999992      2049173    131072.00
     669.183     0.999993      2049174    145635.56
     671.231     0.999994      2049176    163840.00
     674.303     0.999995      2049178    187245.71
     675.839     0.999995      2049179    218453.33
     680.447     0.999996      2049181    262144.00
     680.447     0.999997      2049181    291271.11
     681.471     0.999997      2049182    327680.00
     683.007     0.999997      2049183    374491.43
     685.567     0.999998      2049184    436906.67
     690.175     0.999998      2049185    524288.00
     690.175     0.999998      2049185    582542.22
     690.175     0.999998      2049185    655360.00
     691.199     0.999999      2049186    748982.86
     691.199     0.999999      2049186    873813.33
     692.223     0.999999      2049188   1048576.00
     692.223     1.000000      2049188          inf
#[Mean    =        2.830, StdDeviation   =       11.820]
#[Max     =      691.712, Total count    =      2049188]
#[Buckets =           27, SubBuckets     =         2048]
----------------------------------------------------------
  2099625 requests in 7.00m, 136.04MB read
  Non-2xx or 3xx responses: 2
Requests/sec:   4998.23
Transfer/sec:    331.63KB
```
Результаты тестов в режиме симулирования запросов на чтение демонстрируют разнонаправленные изменения релевантных показателей. Достигнутое уменьшение средней задержки (30%) в кластере с хранением реплик сопровождается двукратным снижением интенсивности операций, аналогичным негативной динамике в наблюдениях за обработкой PUT-запросов. При этом ухудшение статистики распределения в верхнем (начиная с 99,99%) диапазоне квантилей не имеет столь контрастирующего характера, как в серии с добавлением/изменением данных в хранилище: максимальный интервал ожидания ответа при поиске значения среди реплик превышает время отклика в базовой конфигурации без поддержки резервирования в 1,16 раза.<br/>                  
<ins>Flamegraph-анализ</ins><br/>  

![get_cpu_basic_shards](resources/flamegraphs/get_cpu_basic_shards.svg)
<p align="center">Рис.7. Выделение ресурса CPU при симулировании GET-запросов (<em>basic sharding</em>)</p>

![get_cpu_replication](resources/flamegraphs/get_cpu_replicas.svg)
<p align="center">Рис.8. Выделение ресурса CPU при симулировании GET-запросов (<em>replication</em>)</p>

Сравнение распределения процессорного времени при получении данных воспроизводит результаты анализа профилей на основе операций вставки и изменения записей. С учётом логического контекста работы сервера в качестве обработчика GET-запросов используется метод <em>getWithMultipleNodes</em>, запускающий процедуру сопоставления timestamp-атрибутов реплик в локальных копиях LSM для определения актуального значения. Аналогично заполнению БД, в процессе чтения из неё не зафиксировано признаков выполнения операций в асинхронном режиме (путём вызова метода <em>runAsyncHandler</em>).                            

![get_alloc_basic_shards](resources/flamegraphs/get_alloc_basic_shards.svg)
<p align="center">Рис.9. Выделение ресурса RAM при симулировании GET-запросов (<em>basic sharding</em>)</p>

![get_alloc_replicas](resources/flamegraphs/get_alloc_replicas.svg)
<p align="center">Рис.10. Выделение ресурса RAM при симулировании GET-запросов (<em>replication</em>)</p>

Ключевыми драйверами аллокаций в конфигурации с репликами, как и в ходе профилирования на основе PUT-запросов, являются операции с приёмом и парсингом данных из буфера. Перераспределение памяти для поддержки репликаций и проксирования запросов происходит при вызове релевантного действующей логике метода <em>getWithMultipleNodes</em>. 

![get_lock_basic_shards](resources/flamegraphs/get_lock_basic_shards.svg)
<p align="center">Рис.11. Профиль lock/monitor при симулировании GET-запросов (<em>basic sharding</em>)</p>

<<<<<<< HEAD
![get_lock_replicas](resources/flamegraphs/get_lock_replicas.svg)
<p align="center">Рис.12. Профиль lock/monitor при симулировании GET-запросов (<em>replication</em>)</p>
=======
### Report
Присылайте pull request со своей реализацией поддержки кластерной конфигурации на review.
Не забудьте нагрузить, отпрофилировать и проанализировать результаты профилирования под нагрузкой.
С учётом шардирования набор тестов расширяется, поэтому не забывайте **подмёрдживать upstream**.

## Этап 5. Репликация (deadline 2020-10-28 00:00:00 MSK)

Реализуем поддержку хранения [нескольких реплик](https://en.wikipedia.org/wiki/Replication_(computing)) данных в кластере для обеспечения отказоустойчивости.

HTTP API расширяется query-параметром `replicas`, содержащим количество узлов, которые должны подтвердить операцию, чтобы она считалась выполненной успешно.
Значение параметра `replicas` указывается в формате `ack/from`, где:
* `ack` -- сколько ответов нужно получить
* `from` -- от какого количества узлов

Таким образом, теперь узлы должны поддерживать расширенный протокол (совместимый с предыдущей версией):
* HTTP `GET /v0/entity?id=<ID>[&replicas=ack/from]` -- получить данные по ключу `<ID>`. Возвращает:
  * `200 OK` и данные, если ответили хотя бы `ack` из `from` реплик
  * `404 Not Found`, если ни одна из `ack` реплик, вернувших ответ, не содержит данные (либо **самая свежая версия** среди `ack` ответов -- это tombstone)
  * `504 Not Enough Replicas`, если не получили `200`/`404` от `ack` реплик из всего множества `from` реплик

* HTTP `PUT /v0/entity?id=<ID>[&replicas=ack/from]` -- создать/перезаписать (upsert) данные по ключу `<ID>`. Возвращает:
  * `201 Created`, если хотя бы `ack` из `from` реплик подтвердили операцию
  * `504 Not Enough Replicas`, если не набралось `ack` подтверждений из всего множества `from` реплик

* HTTP `DELETE /v0/entity?id=<ID>[&replicas=ack/from]` -- удалить данные по ключу `<ID>`. Возвращает:
  * `202 Accepted`, если хотя бы `ack` из `from` реплик подтвердили операцию
  * `504 Not Enough Replicas`, если не набралось `ack` подтверждений из всего множества `from` реплик

Если параметр `replicas` не указан, то в качестве `ack` используется значение по умолчанию, равное **кворуму** от количества узлов в кластере,
а `from` равен общему количеству узлов в кластере, например:
* `1/1` для кластера из одного узла
* `2/2` для кластера из двух узлов
* `2/3` для кластера из трёх узлов
* `3/4` для кластера из четырёх узлов
* `3/5` для кластера из пяти узлов

Выбор узлов-реплик (множества `from`) для каждого `<ID>` является **детерминированным**:
* Множество узлов-реплик для фиксированного ID и меньшего значения `from` является строгим подмножеством для большего значения `from`
* При `PUT` не сохраняется больше копий данных, чем указано в `from` (т.е. не стоит писать лишние копии данных на все реплики)

Фактически, с помощью параметра `replicas` клиент выбирает, сколько копий данных он хочет хранить, а также
уровень консистентности при выполнении последовательности операций для одного ID.

Таким образом, обеспечиваются следующие примеры инвариантов (список не исчерпывающий):
* `GET` с `1/2` всегда вернёт данные, сохранённые с помощью `PUT` с `2/2` (даже при недоступности одной реплики при `GET`)
* `GET` с `2/3` всегда вернёт данные, сохранённые с помощью `PUT` с `2/3` (даже при недоступности одной реплики при `GET`)
* `GET` с `1/2` "увидит" результат `DELETE` с `2/2` (даже при недоступности одной реплики при `GET`)
* `GET` с `2/3` "увидит" результат `DELETE` с `2/3` (даже при недоступности одной реплики при `GET`)
* `GET` с `1/2` может не "увидеть" результат `PUT` с `1/2`
* `GET` с `1/3` может не "увидеть" результат `PUT` с `2/3`
* `GET` с `1/2` может вернуть данные несмотря на предшествующий `DELETE` с `1/2`
* `GET` с `1/3` может вернуть данные несмотря на предшествующий `DELETE` с `2/3`
* `GET` с `ack` равным `quorum(from)` "увидит" результат `PUT`/`DELETE` с `ack` равным `quorum(from)` даже при недоступности **<** `quorum(from)` реплик
>>>>>>> upstream/master

Регулирование взаимоисключений в практике чтения из набора реплик осуществляется в рамках операций с буфером и при вызове <em>getWithMultipleNodes</em>, включающего значимые для текущего профиля реализации метода <em>invoke</em>. Ранее установленное отсутствие событий синхронизации на уровне экземпляра <em>ArrayBlockingQueue</em> рассматривается как изменение, актуальное и для данного эпизода сравнения конфигураций.     
=======
### Report
Присылайте pull request со своей реализацией поддержки кластерной конфигурации на review.
Не забудьте нагрузить, отпрофилировать и проанализировать результаты профилирования под нагрузкой.
С учётом репликации набор тестов расширяется, поэтому не забывайте **подмёрдживать upstream**.

## Этап 6. Асинхронный клиент (deadline 2020-11-04 00:00:00 MSK)

Переключаем внутреннее взаимодействие узлов на асинхронный `java.net.http.HttpClient`.
Параллельно отправляем запросы репликам и собираем подтверждения на `CompletableFuture`.

Проведите нагрузочное тестирование с помощью [wrk](https://github.com/giltene/wrk2) в несколько соединений.

Отпрофилируйте приложение (CPU, alloc и **lock**) под нагрузкой с помощью [async-profiler](https://github.com/jvm-profiling-tools/async-profiler) и сравните результаты latency и профилирования по сравнению с неасинхронной версией.